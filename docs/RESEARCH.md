# ğŸ”¬ RESEARCH.md â€” Evidence & Pointers

> **Active research into robotics platforms, VLA models, and integration strategies**

## Jetson Thor (deployment)
- NVIDIA page: 2070 FP4 TFLOPS, 128Â GB, 40â€“130Â W; **Jetson AGX Thor**; native **2Ã— CAN** on carrier.  
- Devâ€‘kit carrier spec PDF (AugÂ 2025).  
- Coverage (TechRadar/WindowsCentral/Barronâ€™s/ServeTheHome).

## Robot Platforms & Runtimes

### OpenMind OM1 (NEW - Dec 2024 Investigation)
- **Repository**: https://github.com/OpenMind/OM1
- **Architecture**: Modular AI runtime for multimodal agents across digital & physical robots
- **Key Features**:
  - Multi-platform: Humanoids, quadrupeds, educational robots, apps, websites
  - Multimodal I/O: Web data, social media, cameras, LIDAR â†’ motion, navigation, conversation
  - Middleware support: ROS2, Zenoh, CycloneDDS
  - Hardware: Jetson AGX Orin, Mac Studio M2 Ultra, Mac Mini M4 Pro, RPi 5
  - Configuration: JSON5-based, plugin architecture
  - License: MIT (permissive)
- **Integration with Saorsa**:
  - Complementary: OM1 for high-level orchestration, Saorsa for real-time safety-critical control
  - Bridge via ROS2/gRPC for sensor sharing
  - Potential FFI integration for Rustâ†”Python interop
  - Challenges: Python GIL vs real-time guarantees
  - Opportunity: Use OM1's cloud AI with our local inference

## VLA / ARM models
- **OpenVLA** â€” openâ€‘source VLA (7B), pretrained on 970k episodes (Openâ€‘X Embodiment), code & checkpoints; **OpenVLAâ€‘OFT** shows 25â€“50Ã— faster inference and â†‘success on LIBERO.  
- **MolmoAct (AI2, 2025)** â€” first fully open **Action Reasoning Model (ARM)** â€œthinks in 3Dâ€, plans waypoints; blog & coverage.  
- Related: **Octo** (Stanford), **RDTâ€‘1B** (Tsinghua), **OpenÂ Xâ€‘Embodiment** dataset.

## Stereo cameras (Jetson/macOS)
- **ZED SDK** (Jetson install guide).  
- **Luxonis OAKâ€‘D / DepthAI** (USB; onboard NPU).  
- **Intel RealSense** â€” D455/D555; spinâ€‘out from Intel secured funding (2025), ecosystem active.

## Voice â€” fully local
- **ASR**: fasterâ€‘whisper (CTranslate2, Metal/GPU), whisper.cpp (CoreÂ ML/ANE), Vosk (lightweight).  
- **TTS**: **Chatterbox** (Resemble AI, MIT, 0.5B, emotion control; trending 2025), **Kokoroâ€‘82M** (Apacheâ€‘2.0), **Piper** (fast CPU, many voices).  
- **NVIDIA Riva** runs on Jetson for ASR/TTS (licensing note; optional).

## CAN on macOS
- **pythonâ€‘can gs_usb backend** supports Windows/Linux/**Mac** for candleLight/CANable/CANtact via libusb.  
- **MacCAN** drivers: **Kvaser** and **PCAN** userâ€‘space drivers for macOS (AppleÂ Silicon supported), PCANBasicâ€‘compatible API.  
- **Intrepid ValueCANÂ 4** â€” macOS drivers available; CANÂ FD; isolated.  
- **SLCAN** serialâ€‘line option and libraries (libSLCAN, SerialCAN).

## Protocols & actuators
- **ODrive** CANSimple protocol docs; ROSÂ 2 CAN bridge.  
- **Tâ€‘Motor AK** CAN protocol manuals; community control libs.  
- **CANopen (ros2_canopen)**; **Cyphal (UAVCANÂ v1)** for realâ€‘time distributed nodes (pycyphal).

## Calibration & tags
- **AprilTag** (C, ROSÂ 2, Rust bindings) for fiducials and pose.  
- **OpenCV** stereo calibration tutorial.

---

## Links

- Jetson Thor: NVIDIA page; devâ€‘kit PDF; ServeTheHome, TechRadar, Barronâ€™s, WindowsCentral.  
- VLA/ARM: OpenVLA (GitHub/site/OFT), AI2 MolmoAct, Octo, RDTâ€‘1B, OpenÂ Xâ€‘Embodiment.  
- Cameras: ZED SDK for Jetson; OAKâ€‘D (DepthAI); RealSense news.  
- Voice: fasterâ€‘whisper; whisper.cpp CoreÂ ML; Vosk; NVIDIA Riva docs.  
- TTS: Chatterbox repo & demo; Kokoro (HF & GitHub); Piper.  
- CAN (macOS): pythonâ€‘can gs_usb docs; MacCAN Kvaser/PCAN; Intrepid ValueCAN; SLCAN libs.  
- Actuators/Protocols: ODrive CANSimple; Tâ€‘Motor AK manuals; ros2_canopen; pycyphal.  
- Tags/Calib: AprilTag ROS/Rust; OpenCV calibration.

> See the chat message for inline citations and dates.

## Integration Architecture Patterns

### Hybrid Python-Rust Architecture
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      High-Level Planning (Python)    â”‚
â”‚  OM1 Runtime / Cloud VLA Models      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ ROS2/gRPC
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Mid-Level Coordination (Rust)     â”‚
â”‚  Saorsa Brain Daemon / Safety Guard  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ CAN/Serial
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      Real-Time Control (Rust)        â”‚
â”‚    Motor Control / Sensor Fusion     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Communication Strategies
- **Shared Memory IPC**: Same-machine Pythonâ†”Rust
- **ROS2 Topics**: Sensor data, high-level commands
- **gRPC**: Model inference requests
- **CAN Bus**: Real-time motor control

## Future Research Areas

### Near-Term (Q1 2025)
1. **Unified Robot Description Format**: Extend URDF with VLA metadata
2. **Local-Cloud Hybrid Inference**: Intelligent routing based on latency/complexity
3. **Cross-Platform Skill Transfer**: Abstract skills to capability representations

### Mid-Term (Q2-Q3 2025)
1. **Formal Verification of VLA Behaviors**: Symbolic abstraction of learned policies
2. **Distributed Multi-Robot Coordination**: Blockchain-inspired consensus
3. **Edge-Optimized Models**: 1B parameter models on Jetson Orin

### Long-Term (2026+)
1. **Self-Supervised Robot Learning**: Curiosity-driven exploration
2. **Neuromorphic Computing**: Intel Loihi, IBM TrueNorth integration
3. **Quantum-Enhanced Planning**: Quantum annealing for multi-robot coordination

_Last updated: December 2024_